#!/usr/bin/env python
# encoding: utf-8
"""
background.py

Requirements:
- numpy:        pip install numpy
- matplotlib:   pip install matplotlib (after brew install pkg-config on OS X)
- emcee:        git clone https://github.com/dfm/emcee.git
                cd emcee; python setup.py install --prefix=$VIRTUAL_ENV (or desired prefix)

Created by vallis on 2012-04-10.
Copyright (c) 2012 California Institute of Technology
"""

from __future__ import division
import sys, os, math, random, time, multiprocessing
import numpy as N, scipy.special as SS, scipy.linalg as SL
import emcee

import like
from util import timing
from like import alphamat, Cred_100ns, Cgw_100ns, Cgw_days, Cpn, Gproj, Gdesi, Gdesi2, logL, logL2, simulate, blockmul

def load(challenge,limit=None):
    """Load pulsar metadata and reduced timings, as generated by tempo2/makeres.py and tempo2/makearray.py.
    Leave times in days, convert residuals and errors to units of 100 ns."""

    meta = N.load('../tempo2/%s-meta.npy' % challenge)
    data = N.load('../tempo2/%s-data.npy' % challenge)

    try:
        desi = N.load('../tempo2/%s-desi.npy' % challenge)
    except IOError:
        desi = None

    if limit:
        if data.ndim == 2:
            print "Limit is currently unsupported with two-dimensional data files"
            sys.exit(1)

        if isinstance(limit,list):
            itimes = sum([range(p*data.shape[1],(p+1)*data.shape[1]) for p in limit],[])
            ipars  = sum([range(meta[p]['designpars'],meta[p]['designpars']+meta[p]['pars']) for p in limit],[])

            desi = desi[itimes,:]
            desi = desi[:,ipars]

            meta = meta[limit]
            data = data[limit,:,:]
        else:
            meta = meta[:limit]
            data = data[:limit,:,:]
            desi = desi[:limit*data.shape[1],:meta[limit-1]['designpars']+meta[limit-1]['pars']]

    # unravel data into 'flat' arrays
    if data.ndim == 2:
        times_f = data[:,0]
        resid_f = data[:,1]
        error_f = data[:,2]
    else:
        times_f = data[:,:,0].flatten()
        resid_f = data[:,:,1].flatten()
        error_f = data[:,:,2].flatten()

    # the error generated by the fake/GWbkgrd pipeline seems to be in units of us, not s
    if 'sim' in challenge:
        error_f = error_f * 1e-6

    # rescale residuals and errors to units of 100 ns (yes, hardcoded)
    resid_f = resid_f / 1e-7
    error_f = error_f / 1e-7

    return meta,desi,times_f,resid_f,error_f

def loadraw(challenge,limit=None):
    rawdata = N.load('../raw/%s-data.npy' % challenge)

    if limit:
        rawdata = rawdata[:limit,:,:]

    resid_f = rawdata[:,:,0].flatten() / 1e-7

    return resid_f

def lnprob(x):
    """logL stub function for emcee. It needs to use global variables because otherwise
    multiprocessing.map will need to transmit big arrays to the slave processes."""
    global resid_f,cgw,cpn

    return logL(resid_f,cgw,cpn,A=x[0],n=1,cgwunit='100ns')


# alpha_min, alpha_max = -0.95, -0.55
alpha_min, alpha_max = -0.95, 0.45
# alpha_min, alpha_max = -1.45, -0.05

def lnprob2(x):
    """logL stub function for two-parameter search, also implements alpha prior."""
    global resid_f,alphaab,gmat,meta,cpn

    if alpha_min < x[1] < alpha_max:
        return logL2(resid_f,alphaab,times_f,gmat,meta,cpn,A=x[0],alpha=x[1]) - math.log(alpha_max - alpha_min)
    else:
        return -N.inf


alphared_min, alphared_max = 1.05, 2.45
log10_efac_min, log10_efac_max = -1,1

def lnprob4(x):
    """logL stub function for four-parameter search (including global red noise),
    also implements alpha/alphared priors."""
    global resid_f,alphaab,gmat,meta,cpn

    if x[0] > 0 and (alpha_min < x[1] < alpha_max) and x[2] > 0 and (alphared_min < x[3] < alphared_max):
        return (logL2(resid_f,alphaab,times_f,gmat,meta,cpn,A=x[0],alpha=x[1],Ared=x[2],alphared=x[3])
                - math.log(alpha_max - alpha_min) - math.log(alphared_max - alphared_min))
    else:
        return -N.inf


def lnprob22N(x):
    """logL stub function for (2+2N)-parameter search (including individual red noises),
    also implements alpha/alphared priors."""
    global resid_f,alphaab,gmat,meta,cpn

    Ared,alphared = x[2::2],x[3::2]

    if x[0] > 0 and (alpha_min < x[1] < alpha_max) and N.all(Ared > 0) and N.all((alphared > alphared_min) & (alphared < alphared_max)):
        return (logL2(resid_f,alphaab,times_f,gmat,meta,cpn,A=x[0],alpha=x[1],Ared=Ared,alphared=alphared)
                - math.log(alpha_max - alpha_min) - len(alphared) * math.log(alphared_max - alphared_min))
    else:
        return -N.inf


def lnprob22Nlog(x):
    """logL stub function for (2+2N)-parameter search (including individual red noises with log amplitudes),
    also implements alpha/alphared priors."""
    global resid_f,alphaab,gmat,meta,cpn

    log10Ared,alphared = x[2::2],x[3::2]

    if x[0] > 0 and (alpha_min < x[1] < alpha_max) and N.all((alphared > alphared_min) & (alphared < alphared_max)):
        return (logL2(resid_f,alphaab,times_f,gmat,meta,cpn,A=x[0],alpha=x[1],Ared=10**log10Ared,alphared=alphared)
                - math.log(alpha_max - alpha_min)
                - len(alphared) * math.log(alphared_max - alphared_min))
    else:
        return -N.inf


def lnprob23N(x):
    """logL stub function for (2+3N)-parameter search (including individual red noises and efacs),
    also implements alpha/alphared priors."""
    global resid_f,alphaab,gmat,meta,cpn

    Ared,alphared,log10_efac = x[2::3],x[3::3],x[4::3]

    if (x[0] > 0 and (alpha_min < x[1] < alpha_max)
                 and N.all(Ared > 0) and N.all((alphared > alphared_min) & (alphared < alphared_max))
                 and N.all((log10_efac > log10_efac_min) & (log10_efac < log10_efac_max))):
        return (logL2(resid_f,alphaab,times_f,gmat,meta,cpn,A=x[0],alpha=x[1],Ared=Ared,alphared=alphared,efac=10.0**log10_efac)
                - math.log(alpha_max - alpha_min)
                - len(alphared) * math.log(alphared_max - alphared_min)
                - len(alphared) * math.log(log10_efac_max - log10_efac_min))
    else:
        return -N.inf


def checklike(challenge,procs,yL=500.0,lim=None,gproj=True,inject=False,A=5e-14,alpha=-2.0/3.0,debug=1,prange=None):
    """Load challenge data and compute the likelihood for a range of background amplitudes.
    Returns a two-column array of (A,logL)."""

    global resid_f,cgw,alphaab,times_f,gmat,meta,cpn,error_f

    meta,desi,times_f,resid_f,error_f = load(challenge,limit=lim)
    alphaab = alphamat(meta)

    with timing("Initial setup",1,debug):
        cgw  = Cgw_100ns(alphaab,times_f,alpha,fL=1.0/float(yL))
        cpn  = Cpn(error_f)

        if inject:
            resid_f = simulate(alphaab,times_f,cgw,cpn,A=A,n=1)

        if gproj:
            with timing("Timing for gmat setup",2,debug):
                if desi is None:
                    gmat = Gproj(times_f,len(meta))
                else:
                    print "Using tempo2 design matrix"
                    gmat = Gdesi2(desi,meta)        # gmat = Gdesi(desi,len(meta))

                resid_f = N.dot(gmat.T,resid_f)

                cgw = blockmul(cgw,gmat,meta)       # cgw = N.dot(gmat.T,N.dot(cgw,gmat))
                cpn = blockmul(cpn,gmat,meta)       # cpn = N.dot(gmat.T,N.dot(cpn,gmat))

    pool = multiprocessing.Pool(int(procs))

    if prange is None:
        x = N.linspace(1e-14,9e-14,20)              # range of A
    else:
        x = N.linspace(prange[0],prange[1],20)      # assigned range of A

    with timing("Total timing for {0} likelihoods".format(len(x)),1,debug):
        # l = pool.map(lnprob,[[x0] for x0 in x])
        l = pool.map(lnprob2,[[x0,alpha] for x0 in x])

    pool.close()
    pool.join()

    if debug is True or debug >= 2:
        print "Maximum %s found at par %s" % (N.max(l),x[N.argmax(l)])

    return N.array([x,l]).T


def multicheck(challenge='open1',procs=8,yL=500,inject=True,gproj=False,lim=1,iters=100,prange=None,debug=1):
    ress = []

    for a in range(iters):
        check = checklike(challenge,procs,yL,inject=inject,gproj=gproj,lim=lim,prange=prange,debug=debug)
        ress.append(check[N.argmax(check[:,1]),0])

    return ress


# --- PERFORMANCE NOTES ---
# using MC hammer, http://arxiv.org/abs/1202.3665, http://danfm.ca/emcee
# for our initial runs: each likelihood evaluation takes 6 seconds
# so using 50 walkers x 50 steps / 10 processors should take 1500 s = 25 m
# one iteration = 50 * 6 / 10 = 0.5 m... but they're taking a variable time, 50-95 s
# I should profile the inversion code on multiple processors, to see how it scales!
# 100 walkers x 200 steps = 6 hours. Let's try that.

# after latest checklike experiments: 20 likelihood evaluations take 15 secs on 10 CPUs (ETD) + 21 secs setup (w/gproj)
#                                     20 likelihood evaluations take 16 secs on 10 CPUs (ETD) + 7.5 secs setup (wo/gproj)
#                                     non-ETD numbers are not much worse

def emceehammer(challenge,procs=10,suffix=None,ndim=None,nwalkers=200,iters=100,limit=None,inject=False,resume=False,checkpoint=None):
    """Load challenge data and perform a single-parameter (A) emcee Hammer run on them.
    Save chain and probabilities to numpy arrays."""

    global resid_f,cgw,alphaab,times_f,gmat,meta,cpn,error_f

    meta,desi,times_f,resid_f,error_f = load(challenge,limit=limit)
    alphaab = alphamat(meta)

    if inject == 'raw':
        print "Loading clean data from raw challenge files"
        resid_f = loadraw(challenge,limit=None)

    if ndim is None and challenge in ['closed1','closed2','closed3']:
        ndim = 2*len(meta) + 2

    with timing("Initial setup"):
        cgw  = Cgw_100ns(alphaab,times_f,alpha=-2.0/3.0,fL=1.0/500)
        cpn  = Cpn(error_f)

        if challenge == 'open3':
            cpn = cpn +  Cred_100ns(alphaab,times_f,A=5.77e-22,alpha=1.7,fL=1.0/500)

        if inject == 'inject':
            print "Injecting synthetic signals at dataset times"
            resid_f = simulate(alphaab,times_f,cgw,cpn,A=5e-14,n=1)

        if desi is None:
            gmat = Gproj(times_f,len(meta))
        else:
            print "Using tempo2 design matrix"
            gmat = Gdesi2(desi,meta)        # gmat = Gdesi(desi,len(meta))

        resid_f = N.dot(gmat.T,resid_f)

        if ndim == 1:                       # otherwise the multiplication is done in logL
            cgw = blockmul(cgw,gmat,meta)   # cgw = N.dot(gmat.T,N.dot(cgw,gmat))
            cpn = blockmul(cpn,gmat,meta)   # cpn = N.dot(gmat.T,N.dot(cpn,gmat))

    if N.any(N.isnan(cgw.flatten())) or N.any(N.isinf(cgw.flatten())):
        raise ArithmeticError

    # multiprocessing seems to work better if nwalkers >> procs
    # also keep in mind that the ensemble is split in two...

    trueA, truealpha = 5e-14, -2.0/3.0
    trueAred, truealphared = 5.77e-22, 1.7

    if ndim == 1:
        # initial walker positions - a list of numpy arrays
        p0 = [random.uniform(trueA*0.5,trueA*1.5) for i in range(nwalkers)]

        sampler = emcee.EnsembleSampler(nwalkers,ndim,lnprob,args=[],threads=int(procs))
    elif ndim == 2:
        p0 = [[random.uniform(trueA*0.5,trueA*1.5),
               random.uniform(alpha_min,alpha_max)] for i in range(nwalkers)]

        sampler = emcee.EnsembleSampler(nwalkers,ndim,lnprob2,args=[],threads=int(procs))
    elif ndim == 4:
        p0 = [[random.uniform(trueA*0.5,trueA*1.5),
               random.uniform(alpha_min,alpha_max),
               random.uniform(trueAred*0.1,trueAred*10),
               random.uniform(alphared_min,alphared_max)] for i in range(nwalkers)]

        sampler = emcee.EnsembleSampler(nwalkers,ndim,lnprob4,args=[],threads=int(procs))
    elif ndim == 2*len(meta) + 2:
        p0 = [[random.uniform(trueA*0.5,trueA*1.5),random.uniform(alpha_min,alpha_max)] +
              [value for pulsar in meta
                     for value in [random.uniform(math.log10(trueAred*0.1),math.log10(trueAred*10)),random.uniform(alphared_min,alphared_max)]]
#                    for value in [random.uniform(trueAred*0.1,trueAred*10),random.uniform(alphared_min,alphared_max)]]
              for i in range(nwalkers)]

        sampler = emcee.EnsembleSampler(nwalkers,ndim,lnprob22Nlog,args=[],threads=int(procs))
        # sampler = emcee.EnsembleSampler(nwalkers,ndim,lnprob22N,args=[],threads=int(procs))
    elif ndim == 3*len(meta) + 2:
        p0 = [[random.uniform(trueA*0.5,trueA*1.5),random.uniform(alpha_min,alpha_max)] +
              [value for pulsar in meta
                     for value in [random.uniform(trueAred*0.1,trueAred*10),
                                   random.uniform(alphared_min,alphared_max),
                                   random.uniform(log10_efac_min,log10_efac_max)]]
              for i in range(nwalkers)]

        sampler = emcee.EnsembleSampler(nwalkers,ndim,lnprob23N,args=[],threads=int(procs))

    suffix = (suffix + '-' + str(ndim)) if suffix else str(ndim)

    resumefile = '../runs/resume-{0}-{1}.npy'.format(challenge,suffix)
    chainfile  = '../runs/chain-{0}-{1}.npy'.format(challenge,suffix)
    lnprobfile = '../runs/lnprob-{0}-{1}.npy'.format(challenge,suffix)

    if resume:
        p0 = N.load(resumefile)
        print "Resuming run from file", resumefile

    if checkpoint:
        for subrun in range(int(iters/checkpoint)):
            with timing("{0} x {1} samples (subrun {2})".format(checkpoint,nwalkers,subrun)):
                sampler.run_mcmc(p0,checkpoint)

            p0 = sampler.chain[:,-1,:]
            N.save(resumefile,p0)

            N.save(chainfile, sampler.chain)
            N.save(lnprobfile,sampler.lnprobability)
    else:
        with timing("{0} x {1} samples".format(iters,nwalkers)):
            sampler.run_mcmc(p0,iters)

        N.save(resumefile,sampler.chain[:,-1,:])

        N.save(chainfile, sampler.chain)
        N.save(lnprobfile,sampler.lnprobability)

    print "Done! Mean acceptance fraction:", N.mean(sampler.acceptance_fraction)

    # TO DO: broken in the emcee revision I'm using?
    # print "Autocorrelation time:", sampler.acor


# compare N.dot and blockmul timings
# pulsars  |    5 |   10 |   15 |   20 |    25 |    30 |   all |
# N.dot    | 0.11 | 0.91 | 2.96 | 7.06 | 13.61 | 23.52 | 40.57 |
# blockmul | 0.03 | 0.11 | 0.25 | 0.45 |  0.70 |  1.02 |  1.57 |
# bm w/N   |      |      |      |      |       |  1.18 |  1.79 |
# speedup  | 3.66 |      |      |      |       |       | 25.84 |

def test(challenge,lim=None):
    meta,desi,times_f,resid_f,error_f = load(challenge,limit=int(lim))

    alphaab = alphamat(meta)

    print 'Working with {0} pulsars.'.format(len(meta))

    with timing('GW covariance matrix [recurring]'):
        cgw = Cgw_100ns(alphaab,times_f,-2.0/3.0,fL=1.0/500,approx_ksum=True)

    cgw2 = cgw.copy()
    with timing('Cgw interpolation'):
        cgw3 = 0.2 * cgw + 0.8 * cgw2

    with timing('PN covariance matrix'):
        cpn = Cpn(error_f)

    with timing('Design matrix'):
        gmat = Gdesi2(desi,meta)        # note this takes meta, not len(meta)

    with timing('Reduced data'):
        resid_f = N.dot(gmat.T,resid_f)

    with timing('Reduced Cpn'):
        cpn = blockmul(cpn,gmat,meta,blas=True)

    with timing('Reduced Cgw [recurring]'):
        cgw = blockmul(cgw,gmat,meta,blas=True)

    cgw2 = cgw.copy()
    with timing('Reduced-Cgw interpolation'):
        cgw3 = 0.2 * cgw + 0.8 * cgw2

    with timing('Likelihood [recurring]'):
        logl = logL(resid_f,cgw,cpn)


# recurring costs (MacBook Pro), fL = 1/500
# pulsars     |   15 |   25 |    36 |
# Cgw         | 1.03 | 2.85 |  6.40 | -> with approx_ksum | 0.74 | 2.08 | 4.68 |
# bmul        | 0.17 | 0.45 |  1.02 |
# logL        | 0.36 | 1.49 |  3.36 |
# TOTAL       | 1.56 | 4.79 | 10.78 |                     | 1.26 | 4.02 | 9.06 |    buys 15%
# Cgw interp  |      | 0.14 |  0.47 |
# rCgw interp |      | 0.17 |  0.40 |
# TOTAL  Cgwi |      | 2.08 |  4.85 |                                               buys x2
# TOTAL rCgwi |      | 1.66 |  3.76 | -> may be less stable than interpolating Cgw, buys x3

# conclusion: may be OK to run without interpolation
# timing of two-parameter full-open1 run on octo; 180 s/iteration on 10 processors, 100 walkers
#                                               =  18 s/likelihood, not too bad
#                                               =  10 h/200-step run
# actual timing of two-parameter full-closed1 run: 35153.10 = 17.57 s/evaluation, shy of 10 hr

# older:
#
# for all pulsars and all times:
# pulsars, datapoints: 36, 4680
# covariance:  8.29 s
# linalg.inv: 42.71 s
# scipy.cho:   5.50 s

# notes on resuming:
#   if we run for ~ 10 hours with 200 x 100, then I'd have to split 1000 x 100 in steps of 20
#   python background.py closed3 -p 10 -s long -w 1000 -N 100 -c 20

if __name__ == '__main__':
    import argparse

    parser = argparse.ArgumentParser(description='Run MCMC integration for IPTA MDC')

    parser.add_argument('-p',type=int,default=10  ,help='number of processors')
    parser.add_argument('-s',default=None         ,help='suffix for save file')
    parser.add_argument('-n',type=int,default=74  ,help='number of search parameters')
    parser.add_argument('-w',type=int,default=200 ,help='number of walkers')
    parser.add_argument('-N',type=int,default=100 ,help='number of iterations')
    parser.add_argument('-l',type=int,default=None,help='limit number of pulsars')
    parser.add_argument('-i',action='store_true'  ,help='inject simulated data')
    parser.add_argument('-r',action='store_true'  ,help='resume run')
    parser.add_argument('-c',type=int,default=None,help='checkpoint interval')

    parser.add_argument('challenge',metavar='CHALLENGE',help='challenge name')

    args = parser.parse_args()

    emceehammer(args.challenge,procs=args.p,
                suffix=args.s,ndim=args.n,nwalkers=args.w,iters=args.N,
                limit=args.l,inject=args.i,resume=args.r,checkpoint=args.c)
